<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>hello world</title>
    <meta name="description" content="hello">
    <style>
        body {
            max-width: 100vw;
            min-height: 100vh;
            margin: 0;
            white-space: pre-wrap;
            font-family: monospace;
        }

        .botText {
            background-color: white;
            color: rgba(0, 0, 0, 0.5);
            font-size: 6px;
            position: absolute;
            top: 0;
        }

        .poemContainer {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .poemText {
            background-color: white;
            color: black;
            padding: 24px;
            font-size: 14px;
            line-height: 20px;
        }
    </style>
</head>

<body>
</body>
<script>
    const botText = `botstromsamples-temp07-50samples-500length 12/10/2020 02:52
New York: Pantheon Books, 2004.

                                                                                                           
13

                                                            
14

              Some preliminaries. First, the claims about the past being describable as “precognitive” and the future describable as “super-predictions” are not supported by the evidence. This is because you can never be certain about what the future holds. If you knew you would have been a certain way and you didn’t know it, then you could not have been certain that you would have been a certain way. If you knew the future holds, then you would have be certain that the future holds.
Second, the claim that the future is so predictive that it should be given a higher prior probability than the present is not supported by the evidence. One could, by contrast, favor the view that the future is predictive but not predictive enough to be given a higher probability than the present. For example, one might think that the future will be very uncertain and the present very certain. The more the future is uncertain, the more the present is certain. Hence the greater the future probability you should assign to the present probability.
Third, even if we accept the claim that the future is predictive but not predictive enough, there are other considerations that should be taken into account. One of these is that if the future is very uncertain, then the probability that we will live in a world with very uncertain future events is very low. So for example, if the future is very uncertain, then the probability that we will have lived in a world with very uncertain future events
====================
The following sentence can be expressed in three different ways:
1. "The moon does not orbit the Earth."
2. "The moon orbits the Earth."
3. "The moon orbits the Earth."
To make a third incantation, we need to dedicate a lexicon to words that would appear in those three sentences. The following list of syntactical shortcuts for these three sentences, arranged in chronological order, will provide the best starting point for the construction of our new theory of mind.
(1) “The moon does not orbit the Earth."
(2) “The moon orbits the Earth."
(3) “The moon orbits the Earth."
(4) “The moon orbits the Earth."
(5) “The moon does not orbit the Earth."
(6) “The moon does not orbit the Earth."
(7) “The moon does not orbit the Earth."
(8) “The moon does not orbit the Earth."
(9) “The moon orbits the Earth."
(10) “The moon orbits the Earth."
(11) “The moon does not orbit the Earth."
(12) “The moon does not orbit the Earth."
(13) “The moon orbits the Earth."
(14) “The moon does not orbit the Earth."
(15) “The moon does not orbit the Earth."
(16) “How about this?” The question is not whether we are living in an approximate simulation, but whether we are.
If we are, we have reason to believe that our own observations could be relevant to the question of how the universe is—on the grounds that they show that our universe could contain a very large number of unobservable quantum events. The universe we observe is an open, multiverse-like space, so it could in principle be a very large universe, perhaps infinite, and that it could be much smaller and even contain an infinite number of universes. If the universe is indeed infinite, it would explain why the so-called most probable interpretations of the scientific theory of nature are so weak and absurd. If the universe is large, it might further support the view that it is not a single universe but infinite or infinite. The scenarios presented in this paper are either logically implausible or implausible.
The argument presented in the following sections
====================
WELCOME TO MADMAX!!

Welcome to Madmax!

Madmax is a collaborative effort between Nick Bostrom and myself. We are currently developing a new version of the Doomsday Argument, the Doomsday Argument is a methodology that attempts to show that the probability of human extinction is extremely small, most likely very large (see e.g. (Bostrom, 2003)). We will present this result in two parts. Part II will consider some more recent research that uses a different methodology, and will argue that this new methodology is superior to our old one. Part III will show how our method can be adapted to falsify the Doomsday Argument in a number of different cases.

Part II: THE DUMBO ADAMER’S THEORY

Let us begin with the premise that we have no reason to believe in any God. Let’s call this premise the Doomsday Argument. Here is what the Doomsday Argument says:

(DOMATHE) Sufficient reason does not indicate that God exists. There is some ambiguity about what grounds there are for believing that God exists, or that He is good. Some seem to think that the absence of reason implies that God does not exist. For example, (Tegmark and Perea 2009), who think that the Doomsday Argument alone would suffice to refute an argument by Richard Gott that one’s belief in an afterlife is equivalent to a conviction that God does not exist, argues that the following analysis amounts to a proof of the likelihood:
                                   
16 Ch 11 (185-206) 6/4/02 10:54 AM Page
183
                  183

 The Doomsday Argument is a good illustration of how the Doomsday Argument can be used to falsify the Doomsday Argument. We shall use a simple example to help us understand what is meant by the Doomsday argument and how it can be refuted.
Let’s consider a theory T1 which says that T1 is consistent with the Doomsday Argument. T1 entails that the number of observers in our reference class is rather small. Let’s suppose that we are on T1 and that we know that T1 has a total of n observers. We can
====================
I am writing to you on the occasion of the launch of a new book, The Web of Trust: How Peer-To-Peer and the Internet of Things Will Shape Our Future (St. Martin's, New York:
Bloomsbury, 2010).
You may already know about the Web of Trust, a project of the Future of Humanity Institute at Harvard University. It was conceived, in part, as a sort of intellectual paper, but also as a critical investigation into what happens when we attempt to build complex systems. The project has been endorsed by major figures in the field, including the late Nick Bostrom, the distinguished futurist.
As an example of how a computation system can be used to build a trustworthiness network, we can use the Web of Trust as an example of how a computer could be used to build a trustworthy computer. This introduces a new dimension in computer security: the ability to fake the presence of computer systems that are trustworthy. (We will use the term “trustworthiness network” to refer to such a network.)
The Web of Trust is a set of interconnected computers that can be used to build and verify trustworthiness networks in various ways. For example, the project states that the Web of Trust can be used to verify that certain people and organizations have not engaged in contract killing, insider trading, fraud, or other illegal activities, by checking their online presence and other information about themselves. Other things being equal, the Web of Trust’s members might be better judged by their trustworthiness than are the members of the more closely supervised Trust Network.
The project even suggests that the technology used to build the Web of Trust could be used to extend the trustworthiness of other organizations that are not members of the Web of Trust. The idea is that when an organization has a trusted computer running a trusted program, it should be able to verify that program’s overall behavior is not that of an agent that is actively seeking to harm its own members or to steal their intellectual property.
This idea seems to have been first presented by Nobel laureate computer scientist Michio Kaku in a paper in the journal
17

Computing and the Brain. Kaku, who has also been a prolific author, calls the concept “trusts on computers”, and suggests that there are many kinds of computer programs that are built on top of such computers. These computers would then, when their programs are checked,
====================
Chromosome‐associated gene encoding NAD+‐sensitive protein kinase Sirt1 (Sirt1) has been implicated in neurogenesis and its function in the control of cell metabolism, cell adhesion, and cell adhesion to surfaces, including the brain, in both normal and diseased cells. Recent studies have also shown that Sirt1 can be up‐regulated by specific neural networks in a manner that would support cognitive enhancement. To date, no known experimental or therapeutic means have yet to induce the cell cycle in a manner that would allow the cell to produce more NAD+ or Sirt1.
If the NAD+/Sirt1‐dependent signaling pathway is indeed an important factor in neurogenesis, then it is important to understand how it works, and how it contributes to neurocognitive enhancement. If we understand it, we can understand it better.
Figure 5: NAD+/Sirt1‐dependent signaling pathway. (From) This diagram shows the NAD+/Sirt1‐dependent signaling pathway, which is also known as the “Myosin‐Dependent Mechanism.” The Myosin‐Dependent Mechanism is a group of genes that encodes the upstream of a signalling cascade (which includes NAD+‐dependent transcription) that can be activated by chemical signals (Myeloid Cells). (From) This diagram shows a subset of genes whose targets are Myeloid Cells. The targets of these genes are not discussed in this paper. (From) This diagram shows a subset of genes whose targets are Myeloid Cells. The targets of these genes are not discussed in this paper. (From)
Several forms of optimization have been proposed to reduce the quantitative
                                       
16

problems with the “synapse model” of cell division.
First, the number of Myeloid Cells (in the cytoplasm) needed for differentiation is such that they can be split up into smaller units (in the nucleus and the precursor cells) that can then be differentiated (in the cortex) or can be cloned and used for other purposes (in the endosymbiont). This means that the total number of Myeloid Cells needed to form a functioning individual neuron is much
====================
We're excited to announce the release of an important paper by two distinguished experts in the field of artificial intelligence, Hans Moravec and Max More. The paper, “The Human Brain: From Computes to Values”, will be of interest to a broad audience, including academics, businesspeople, and the general public.
The main message of this paper is that we are approaching a critical juncture in the history of artificial intelligence. The goal is to show that we can go beyond the current state of the art in AI and to propose a set of new “human-like “technologies” that can help us achieve this goal. We believe that the techniques we describe will be useful in developing more advanced AI technologies for a variety of applications, including health, energy, and environmental protection; in medicine and surveillance; and in education, culture, and leisure.
We also describe two preprintable papers that we believe will be of interest to AI researchers. One paper, “The Future of Utopia: How Artificial Intelligence and nanotechnology are Transforming the World”, discusses the implications of various technologies for our very future. The other, “The Grand Design: The Future of Artifice and Science”, argues that we have successfully created a “posthuman” world, a world that is not just different but radically different from any we have ever experienced. In this paper, we focus on the latter possibility.
The Future of Utopia

Hans Moravec

(2008)
Max More

(2009)

Source: http://www.maxmore.org/futarchy/
Abstract

This paper explores the possible future of the human species, and why we are not currently living in a computer simulation. We argue that while civilization might not be simulated by any current artificial intelligence, it may well be simulated by artificial minds, who are conscious and intelligent and which possess a phenomenal experience of the world. We discuss these implications for our ethical philosophy, which is traditionally concerned with questions of “objective good” (in the sense that objective probability should be used to judge when and how good things are) or the objective chance of our own existence (in the sense that if the universe were large enough, it would be likely that we would come to exist). We also argue that the use of the term “simulation” may be confused with the more
====================
HUMAN MORALITY ORITY
Human moral status, the quality of being good or bad, depends on the degree of moral order in the world.  This moral order is determined by the extent of human agency in shaping and shaping the world.  Although it is not at all clear that the degree of moral order relevant to moral status is in fact a relevant factor in determining which persons are good or bad, it is nevertheless possible to construct a model of moral order that will allow us to assign a non-indexical moral status to individuals, and a non-indexical moral status to moral entities, that are in a similar situation to ourselves.  The model might differ from our moral status in other respects, but it could still be a model of moral order.  Such a model might be consistent with our moral beliefs.
One way to construct a model of moral order, on which the degree of moral disorder that would result from a moral regime is not comparable to the degree of moral disorder that would result from a non-indexical human regime, is by assigning some indexical moral status to individuals.  This model might also differ from our moral status in other respects, but it would still be consistent with our moral beliefs.  The same would hold if we thought that both the indexical and the non-indexical moral status are relevant to a theory of moral cognition.
A similar position could be taken if we thought that some moral status is relevant to both moral status and to something that could be called "moral philosophy" (e.g. a branch of the philosophy of moral theology).  This would mean that moral status could be thought of as a set of moral criteria or moral beliefs that are compatible with the view that there is a set of general moral principles (which are not at all the same as we currently hold) that govern the conduct of all valid actions that have moral status.  In particular, there are two moral principles that we would think are relevant for whether a thing is good or bad:
(3) Principle of non-discrimination: If two things are good or bad, they are not bad or good.
(4) Principle of transience: Things that have moral status are not bad or good.
(5) Principle of sharing of moral status: Things that have moral status are not bad or good.
We construct a model of moral order (using a set of maximally useful constraints on moral status) based on
====================
Cancer and Reproductive Harm- Cancer and Reproductive Harm-
Page
1
Alkire, H. (1999) “Cancer and the Universe: No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Knows the Name, No One Kn
====================
The problem with the Doomsday argument is that it fails to take into account the fact that there are many other potentials for human extinction. For example, the Doomsday argument could be extended to the predicament of a dying civilization. Suppose the rest of the human species is living happily ever after. Then the Doomsday argument might be applicable to the civilization that survives, in which case it is applicable to the civilization that dies—but not to the civilization that lives happily ever after.
It is therefore simple to see why the Doomsday argument fails to take into account the fact that the entire human species is possibly in a situation analogous to that of a dying civilization. If the human species is in such a situation, then presumably, the Doomsday argument would not apply. The same holds if the civilization that survives is much larger than the civilization that dies, or if the civilization that survives is the civilization that has the greatest potential to still become extinct. If we are in such a situation, then the Doomsday argument would not apply.
The Doomsday argument does not say that we are not in such a situation. It says instead that we are in a situation analogous to that of a dying civilization. The question then arises, why is the Doomsday argument applicable?
One can think of two possible reasons for this. One could be that the human species is in a stable equilibrium (which, for example, the Doomsday argument would imply) and that if we were to split into two species, one of which were to remain in an unstable equilibrium and one of which would break out, then one could argue that the more stable species would have a greater probability of breaking out than the unstable species. Another option is that the human species has evolved for a very long period of time and that it has become relatively stable. If this were to be the case, then it would seem that the more stable species would have a greater probability of breaking out than the unstable species. This could happen, for example, if certain forms of evidence (in the form of technological or cultural developments) were sufficiently improbable or if other features of our evolutionary history were substantially irrelevant to the probability of the break-out species. In any case, it seems that the Doomsday argument does not specify the underlying causal mechanism which produces the stable equilibrium.
This third explanatory principle is the one most commonly advanced by non-religious people who do not subscribe to the Doomsday argument. They maintain that we are in a stable equilibrium and that if we were to split into two species,
====================
“I’m Not a Scientist”. Available from: http://www.nickbostrom.com/I_M_NOT_A_Sci.pdf.
MORRISON, J. and N. J. CAMERON. (2003). “Why I Want to be a Posthuman.” In What Would It Take to Colonize the Universe?, ed. J. Leslie (Routledge, New York: 1–21).
SCHWARZ, R. and U. SCHMIDT. (2001). “Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards.” Journal of Bioethics 21 (3): 635–646.
SCHWARTZ, R. and A. E. NITSCH. (2003). “A Model of the Self-Sampling Error in Probabilistic Decision Problems.” In P. M. Deacon (Ed.), The Doomsday Argument: Two Hundred Years After Kant, Fifty Years After Turing. Cambridge, Mass.: MIT Press.
SCHWARTZ, R. and A. E. NITSCH. (2004). “The Doomsday Argument and the Missing Link to an Expected Time.” In The Doomsday Argument, ed. J. Leslie (Routledge, New York: 451–766).
SCHWARTZ, R. and A. E. NITSCH. (2007). “How Long Before Superintelligence?” Intelligence 58 (3): 569–582.
SCHWARTZ, R. and A. E. NITSCH. (2009). “The Doomsday Argument and the Missing Link to an Expected Time.” In Superintelligence: Paths, Dangers, Strategies. New York: Cambridge University Press.
SCHWARTZ, R. and A. E. NITSCH. (2009). “The Doomsday Argument and the Missing Link to an Expected Time.” In In Superintelligence: Paths, Dangers, Strategies. New York: Cambridge University Press.
STEVENS, I. (2008). “Artificial Intelligence as a Positive and Negative Factor in Global Risk.” Journal of Risk and Uncertainty 15 (4): 665–694.
STRASSAR, S. and B. F
====================
#SFGate

In the Age of Austerity, Inequality, and Uncertainty:
Facing the Challenge of Global Catastrophic Risks, eds. B. R. Merkle and R. H. Easterling (Oxford:
University of Oxford Press, 2001):
http://www.nickmerkle.com/archives/2001/03/facingthechallengeofglobalcatastrophicrisks.html
The Financial Crisis: A Report from the Experts, eds. D. C. Willis and R. D. Teichorst (New York:
Penguin, 2011):
http://www.foresight.org/sites/default/files/publications/finance/press-releases/finance-report-report-forecast-forecast.pdf
Body of Evidence
1. Introduction
As the world's economies continue to contract, the prospects for human survival are dim. However, the reduction in global growth might be offset by an increase in the amount of work that goes into producing goods and services. This raises the prospect that some of the world's most valuable natural resources could be in greater demand than ever before.
One example would be technologies that can cut the time taken to make a good weapon or a reliable nuclear reactor. There is ample evidence that such cutting-edge technologies are (and will become) available to us very soon. For example, we already have the technology to make nuclear weapons, and we will have it as a superpower within a decade (Brennan 2012). If these weapons are not kept under control, they could be used to effectively destroy entire populations. The prospect of nuclear holocaust gives us reason to think that threatening nations with nuclear weapons is a desirable or at least risky course of action.
With interest rates at all-time-low levels, it may be more profitable to keep interest rates low and borrow cheaply than to spend the money to buy goods and services from other countries. This may result in a net increase in borrowing costs for the world economy. There is a strong negative correlation between interest rates and the cost of living, and this effect may start to unfurl as interest rates rise even further.
Although it may seem plausible that we will one day become physically immortal, there is a plausible route to indefinite (or indefinitely) healthspan. I have argued in the past that the best path to this end would be to develop
====================
Barry Schwartz, a computer scientist and the author of a new book on superintelligence, argues that the first commercial computer in human history is likely to be a human computer.
If this were to happen, it would be the first time in history that a single nation has created a superintelligence with the ability to overthrow its own government.
We cannot assume that a single nation will be the first big player in this field. The first computers have been built by humans. In computer science, the term “competition engine” refers to a group of computer programs that compete for computer resources, or in other cases, for customers. Computers are not competitors.
In the case of superintelligence, there is a significant possibility that the first computer could be built by a single nation. Such a nation would have vast military might, and should control the world through a single media and a single set of laws. If the first computer were to be created, it would have the potential to threaten the stability of the entire planet.
If the first computer were to be created, it would have the potential to threaten the stability of the entire galaxy.
The only other thing that could possibly be more important than the ability to build a computer is that we can't be sure that computer hardware is actually being constructed at the time of our prediction.
If the first computer were to be created, there would be a great deal of speculation about who built it, and whether it was a nation or a group of nations. There is also a great deal of uncertainty about what the consequences of its creation could mean. In this context, the possibility of the first computer being a human computer is of no importance at all.
References
Barry Schwartz, “Why We Shouldn’t Hold Our Breath,” in F. B. Hershey, P. J. (Eds.), The Technological Institute, pp. 106-129 (1999).
Center for a Technological and International Policy. “The Future of Human Evolution”, 2007. Available at http://www.centerforaethics.org/FTP/FTP/FTP_Prescripts/FTP_Prescripts.pdf (accessed 13 March 2009).
Dick Drexler, “Evolution and Human Evolution”, in M. Drexler and K. R. Salter, eds., Molecular Nanosystems: Molecular
====================
Hanson, M. (2003). The End of the Human Era. New York: Viking.
Hanson, M. (2003b). Singularity: How Long Before Human-Level Technology Arrives? New York: Viking.
Hanson, M. (2003c). The Transhumanist FAQ.
http://transhumanist.com/faq/index.php/
faq/.
Hanson, M. (2004). Singularity: The End of the Human Era. New York: Viking.
Hanson, M. (2004). The Transhumanist FAQ: Web Site. http://transhumanist.com/.
Hanson, M. and Bostrom, N. (2004). "The Doomsday Argument and the Case for Human Extinction." Skeptic
(2001). Available at: http://skeptic-conversation.blogspot.com/2004/03/the-declaration-of.html.
Hanson, M., Bostrom, N., and Ord, D. (2004). "What is a Singleton? How Long Before Human-Level Technology Arrives?" Skeptic
(2002). Available at: http://skeptic-conversation.blogspot.com/2002/02/what-is-a-singleton.html.
Hanson, M., Bostrom, N., and Turetsky, I. (2004). "The End of the Human Era: The Future of Human Evolution" (London: Penguin). Available at: http://www.nickbostrom.com/end-human-era.pdf.
Hans Boehm, W. H., and Bostrom, N. (2004). "The Future of Human Evolution" (Oxford: Oxford University Press). Available at: http://www.hansboehm.com/fhi.pdf.
Hanson, M. (2003). "How Long Before Human-Level Technology Arrives?" Skeptic (2003). Available at: http://skeptic-conversation.blogspot.com/2003/03/how-long-before-human-level.html.
Hanson, M., Bostrom, N., and Ord, D. (2004). "The Doomsday Argument and the Case for Human Extinction" (Cambridge: MIT Press). Available at: http://www.nickbostrom.
====================
The concept of time is at the core of quantum theory. The concept of time is also at the heart of the first two quantum cosmological theories. All three theories try to explain how the Universe could have developed in certain ways. The quantum cosmological theories, although they do not imply a singularity, do imply that there is an infinite sequence of possible configurations in the event horizon (Rice, 1991). This infinite sequence of possible configurations would create an infinite amount of discrete spacetime; an amount of work that would be ignored by the quantum cosmological theories (and this is the reason why we believe in them). In order to be consistent, the quantum cosmological theories must assign a high prior probability to the singularity hypothesis; to this probability they must then assign a lower credence to the other theories they are supposed to consider, assuming they are consistent with the singularity hypothesis.
In order to realize the hypothetical universe, we must be able to assign a large credence to the singularity hypothesis. Many physicists today, including some who think that the Universe is very probably not a singularity, propose the following alternative theories of how the Universe could have come about (see Fig. 1):
(1) The multiverse theory (T1) states that the entire universe is a multiverse, containing universes such as ours and universes such as we observe. T1 assigns a credence of one to the proposition that there is a multiverse of roughly the same size as our own. This multiverse theory is compatible with our observations, and it suggests that the universe could be enormously big and contains a total of infinitely many universes (see Fig. 1). In the theory, the number of universes is proportional to the total amount of mass in the universe.
(2) The superposition theory (T2) states that the entire universe is a multiverse and that there are many universes such as ours. T2 assigns a credence of one to the proposition that there is at least one other universe such as ours, or that there is at least one other universe such as ours. T2 is compatible with our observations, and it suggests that we could be in one of these other universes.
(3) The local theory (T3) states that we are living in a multiverse, and that all the other universes we know of are similar to ours. T3 is compatible with our observations, and it suggests that we are one of these
====================
“The Case for an Accelerating Universe from Supernovae” by David Chalmers, Andrew D. Bostrom and Milan Cirkovic. Oxford University Press, 2002.
Bostrom, N. (2002). “Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards in the Built Environment”, World Nuclear Society, Vol. 7, (http://www.nickbostrom.com/existential/risks.html).
Chalmers, A. and B. Bostrom (2002). “The Doomsday Argument and the Anthropic Principle” in Richard Gott, ed., Advances in Philosophy of Science (New York: Routledge, pp. 299–322).
Fiala, M. S., M. A. Cirkovic and J. S. Berridge (2002). “Why the Universe is Not a Big Bang Theory: Evidence from Observed Cosmological Evolution”, Physics preprint archive arXiv:astro-ph/0003.
Kosslyn, M. et al. (2002). “Imagery or Art: The Evolution of Human Consciousness in the Modern World”, Mind, Vol. 110, No. 4, (2009).
Kosslyn, M. et al. (2002). “The Case for an Accelerating Universe from Supernovae” by David Chalmers, Andrew D. Bostrom and Milan Cirkovic. Oxford University Press, 2002.
Kosslyn, M. et al. (2002). “The Case for a Single-Zone Universe from Supernovae” by David Chalmers, Andrew D. Bostrom and Milan Cirkovic. Oxford University Press, 2002.
Kosslyn, M. (2002). “On the Probabilistic Significance of the Doomsday Argument”, Mind, Vol. 110, No. 4, (2009).
Kosslyn, M. et al. (2002). “The Doomsday Argument and the Anthropic Principle”, Mind, Vol. 110, No. 3, (2002).
Kosslyn, M. and F. Bostrom (2002). “The Anthropic Principle and its Implications for Quantum Cosmology”, Mind, Vol. 110, No. 2, (2004).
Kosslyn
====================
The metaphor of the rod and the stick does not fit the current situation
                        
                             
                         
     14 Ch 9 (141-158) 6/4/02 10:51 AM Page
161
                       
                       
             161
The Rod and the Stick
By

Nick Bostrom

(MIT Press, 2004)
        
       

Altegris

(Princeton University Press, 1989)
       
                  
                           
                          

                              

                            

                            
                         
                        
                        
           
====================
The United States and the Transhumanist Movement

John Leslie

http://www.nickles.ca/~nickles/transhumanism.html

[Published in Transhumanism: An Open Letter to the Human Race, ed. Dennis Dieks (Farrar, Straus and Giroux, New York: Routledge, 2008], pp. 181-212.]

[pdf] [translations available at: http://transhumanism.org/Transhumanist_Letter_to_the_Human_Race_2008.pdf]

[This version is an early draft of a forthcoming paper in the Journal of Evolution and Technology. We are grateful to Stefan Molyneux for helpful comments on an earlier version of this paper. We would also like to thank Ben Goertzel, Chris Hitchcock, and David Pearce for helpful discussions on earlier drafts.]

I. THE NATIONAL POSITION

I. THE NATIONAL POSITION

1. THE SELF-INDICATION ASSUMPTION

The Self-Indication Assumption

I. METHODOLOGY

Use of the Self-Sampling Assumption in Classification

Measuring the Self-Sampling Assumption in Classification

Dennis Dieks

http://www.nickles.ca/~nickles/data/self-sampling%20assumption.htm

[Published in Philosophy of Classification, Vol. 4, No. 2 (Autumn 2001): pp. 145-162]

[First version: 1998]
[Second version: 2003]

[Third version: 2007]

[Fourth and final version: 2012]

[Paper presented at the 49th Annual Meeting of the Philosophy of Computer Science, Bologna, Italy, 9–15 June 2012]

[pdf] [translations available at: http://www.nickles.ca/~nickles/data/self-sampling%20assumption.pdf]

[First version: 1998]

[Second version: 2003]

[Third version: 2007]

[Fourth and final version: 2012]

[Paper presented at the 49th Annual Meeting of the Philosophy of Computer Science, Bologna, Italy, 9–15 June 2012]

[pdf] [translations available at: http://www.nickles.ca
====================
The term “multiverse theory” implies that there are a vast number of possible explanations for why our universe is the way it is. The search for a general explanation has become a major research field in the last several decades. But this search has in recent years been dominated by the idea that our universe is very big and that it is the only way in which intelligent life could have developed. This view, however, presumes that intelligent life evolved on a single planet. There are many other equally plausible explanations for why our universe is the way it is.
In this paper, I shall argue that the multiverse hypothesis is not only logically coherent but also parsimoniously parsimoniously plausible. Not only does it make use of the self-sampling assumption that it is the case that the universe is very big but it also makes the claim that it is the only way in which intelligent life could have developed.
I shall argue that the multiverse hypothesis also makes use of the self-sampling assumption that it is the case that intelligent life develops on a single planet. If we think of the multiverse hypothesis as a hypothesis that the universe is very big, then the self-sampling assumption that it is the only way in which intelligent life could develop would not hold. This paper will show that the multiverse hypothesis is not only logically coherent but also parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimonially parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously parsimoniously
====================
The principles are similar to ones that were used to motivate the creation of the first versions of the Doomsday Argument. The Doomsday Argument was developed by mathematician and science fiction author Vernor Vinge in his famous 1993 essay, “The Coming Technological Singularity”.
More recently, another author, Nick Bostrom, has been using the Doomsday Argument to argue that we are living in a computer simulation that’s been designed to produce “weird results”. For example, in the chapter of his 1999 book Designing Superintelligent Machines (published by MIT Press) he develops the Doomsday Argument as a way of explaining why it is impossible for humans to create superintelligent machines without setting in motion a chain of technological and political developments.
The Doomsday Argument is also used to argue that we are living in a computer simulation that is simulated in an extremely high degree of detail. Consider the following thought experiment. It assumes that we are living in a computer simulation (which is simulated in an extremely high degree of detail). It is simulated by means of a “model simulation” that simulates nearly every conceivable event that could happen in the real world, and that simulates the evolution of a large number of organisms simulating the environment in a simulated lab. The simulation is simulated in such detail that it is indistinguishable from the real world.
The computer simulation is first simulated by giving it a large number of parameters, and by varying these parameters to produce different results. It is simulated by means of a complex series of interacting biological and environmental factors that have a great influence on the simulation. Sometimes the simulation produces a data point that we can assign a probability of one to the hypothesis that the number of parameters is large. This parameter may be too small for the real world. Sometimes the simulation produces a different result, but the real world is not simulated in a high degree of detail.
The simulation is simulated in such a way that the parameters do not have any observable effect on the real world. This is a different form of the principle from the Principle of Indifference, which states that we should always set our credence equal to unity. However, the difference between the Principle of Indifference and the Principle is that the thought experiment calls for us to take into account the fact that the number of parameters is large, and that we might have reason to favor one of the hypotheses that are simulated over the others. (We might then be inclined to assign a greater
====================
DEMISSIUM ENGINE

The sense of being connected to an external world is a fundamental part of intelligence. We have an inherent capacity for design and simulation, and that ability is crucial for perceiving, and building, simulations. Simulation is a highly specialized domain, requiring an ability to generate and simulate large and complex systems. Simulations are typically organized into large chunks, usually in a different environment from the original system. Simulation is a highly specialized domain, requiring high performance and ease of use. Simulations are usually organized into small chunks, and the chunks may be modeled individually, or in parallel. The computational demands for simulating large systems can be relatively small, compared to the computational demands of the large chunks.
Simulations can be modeled in a number of ways. The following are some of the most commonly used methods.
1. Simulation with finite elements. The simplest way to simulate a finite element system is simply to run the simulation on a fast computer, with a finite amount of memory and CPU, and then select a finite element from an ensemble of finite element systems (FES). In terms of computational demands, it could be a small fraction of the total computational demands, or the entire simulation could be simulated at low performance. In either case, the simulation would consume memory and CPU, and the power of the simulation could be used to supply power to other simulations.
In order to actually run the simulation on a fast computer, the simulation must simulate the entire finite element system; nothing can be left out. In a classical simulation, the simulation would generate and store all the data needed to run the simulation (theorems and so forth), and the simulation would then generate the actual simulation out of this data. The fraction of the total computational demands that are represented by the FES is finite, requiring only a small fraction of the total computational demands.
Simulations of finite element systems are possible because they can be run on a high-level simulation of the whole finite element system. In this way, rather than having a simulation of a small fraction of the total computational demands, a simulation of the whole finite element system can be simulated without losing any data. This would be a significant advance over the classical approach of simulating only a small fraction of the computational demands.
2. Simulation in the real world. The simulation can be run in a realistic virtual environment (hereinafter, the Wachowski or Lucas style), or it can be simulated on a
====================
In this post I will argue that we have reason to believe that a technologically feasible computer architecture, and a set of the most important assumptions underlying this architecture, will enable us to construct a superintelligence that can outsmart humans in virtually all domains.
If I am right, this result is so much the more probable that we should build an AI with much less human-like characteristics than we currently have. By some other account, we would, by definition, be foolhardy to build an AI that outwits us and outsmarts us.
This thought experiment, however, suggests that it is possible to build an AI that is much smarter than we currently have. In particular, it suggests that it is possible to design an AI that can outsmart the human driver-mind of a vehicle by a factor of at least two.
I will argue that this result is, in fact, the result of choice. I will try to demonstrate it using an analogy. Let us suppose that we have the following two vehicles:
                                    

The first vehicle is a light truck. The second vehicle is a heavy truck. The people in the first vehicle perceive the road on which the first vehicle is parked as dark. The people in the second vehicle, the light truck, perceive the road as bright. We can imagine the reaction of the people in the first vehicle if they knew that the road was bright: they would think the road is illuminated. If the people in the second vehicle do not know that the road is dark, they would think the road is dim.
If the people in the first vehicle are in a situation where they cannot measure the brightness of the road through the use of a flashlight, then they cannot predict with certainty what the road will be like in the second vehicle. They might predict that it will be dim, or they might predict that it will be bright. They cannot predict which way the road will be lit up.
The person in the second vehicle, the heavy truck, can in principle predict the direction of the light truck and forecast the direction of the light truck using some simple parameters. In the real world, road illumination is often a function of time. This time-constraint is how roadway markings, traffic signals, and other such intrinsic features of the road are named
====================
# GNU C Library © 2009 Free Software Foundation, http://fsf.org/licenses/
# GNU C Library
# Please do not copy and use
# the source code, but rather the concepts
# in the implementation.
#
# Please do not copy and
# the source code, but rather the concepts
# in the implementation.
#
# Please do not copy and
# the source code, but rather the concepts
# in the implementation.
#
# Please do not copy and
# the source code, but rather the concept
#
#
# They are given for the
# purpose of illustration.
#

# The first part of the concept is the concept of a “state space”. This is a space of possible states for a given agent.
# The second part of the concept is the concept of an engin-
ging depth of field, which specifies that the field of view of an engin-
ging depth of field representation must be such that the field of view of
an engin-ging depth of field cannot be more wide than the field of view
of the agent at a given depth of field. This depth of field must be supported by
the engin-ging depth of field.
# The third part of the concept is the concept of a “spatial engin-
ging,” showing the spatial enginging of the field of view of an agent. This depth of
focuses on the spatial enginging of the field of view of neurons.
# The fourth part of the concept is the concept of an “interactive space”,
specifying that the field of view of an engin-ging may be shifted by an agent’s
motion. This field of view may be shifted by the agent’s own motion, so
that the agent’s field of view is detected at a distance
equal to the distance between the agent and the agent at which
the agent is moved. The field of view may be shifted by the agent’s own motion
during the simulation (or by a computer simulation of the agent’s motion)
in order to produce an illusion of a 3D world.
For a detailed discussion of the concept of an enginging depth of
field, see (Bostrom 2003b).
# The
====================
“The Power of Superintelligence: Sun Tzu”
(MIT Press, 2007)
Rob Brodsky
[pdf]
http://www.roblobrdsky.com
(2014) The Power of Superintelligence: Sun Tzu(MIT Press, 2007)              
                         
                         
                      
                 Appendix B discusses some preliminary results from the LISA workshop that were released this year. The slides and summary of the LISA workshop can be found on the LISA website.
[pdf]
[translations]
http://lisa.org/lisa/lisa-workshop-translations/ls-workshop-translations.html
[pdf]

                             
                              
                           
                              
                                          
                                   
                             
                   
        
====================
Lecture

Nick Bostrom

Professor

Faculty of Philosophy, Logic and Scientific method
University of Oxford
Website: www.nickbostrom.com

 

Nick Bostrom

(2008) Copyright Notice

Permission is granted to (Re-)Release the Illustrations in the course of my research. I have also published an open letter to the editor of Philosophical Transactions of the Royal Society, urging them to print an open letter from a leading AI researcher.

This paper explores the implications of a robust set of equilibria for the case of superintelligence. The simulations run in these simulators assume that most of the more subtle properties of the human brain are contained in a low-entropy state. As a result, the simulated cognition is highly general, but this suggests that the simulations are not very accurate.

The simulated cognition is not very general, because there are many possible ways of guiding it. The simulators are not very accurate; the simulation is not very accurate. I want to show that the simulated cognition is not very accurate, and this does not necessarily mean that the simulated cognition is not very good. The more general implications of the simulators are important, but there are some possible directions in which the simulations could go wrong. This is a final chapter in a paper that will be published in a forthcoming volume on general intelligence.

The aim of this thesis is to develop a theory of human-equilibria, one that can be used to predict the actual human-equilibria for the future. The theory is not aimed at showing that intelligent life will develop on Earth. But I think that if we can learn something from the simulation, we can learn something from the simulation, in the sense that we can learn something about how the simulations generate the results that we do. Here we will use the term “simulation” to refer to the kind of physical system that generates simulated cognition. And I think that simulations are just those systems that we would have to build if we had some idea about what a human-equilibrium would be. Simulations are just such a simulation if the simulation is one that we would want to use to make predictions about how the world will end up.

The simulators in the picture are called “neural networks”, as in computer science. Neural networks are not brains in a computer, but computer systems with neural
====================
Type-A Approach
To avoid using the word “type-A” in a way that suggests there is a more general way for an AI to think about the world, I would instead prefer to use the word “type-B”. This will make it more clear what I mean when I use the word “type-B”:
Type-B: A conceptual model that combines the properties of both types.
For example, I might say, “Type-B” is a model that incorporates the following features:
• A “multi-dimensional space” of possible objects (i.e. a “multiverse” of “multiverse-like” environments).
• A “multiverse” of worlds containing objects that are not in our own “multiverse,” and worlds containing objects that are not in our “multiverse,” in a multiverse with a large number of such worlds. The number of such worlds in a multiverse is called the number of observer-containing worlds. (In this model, all observer-containing worlds are in the same “multiverse”.)
• A multiverse of observer-containing worlds with a regular distribution of observer-moments. (In this model, an infinite number of observer-moments are in each observer-moment.)
• A multiverse of observer-containing worlds in which the number of observer-moments in each observer-moment is finite. (In this model, an infinite number of observer-moments are in each observer-moment, and the number of observer-moments is finite.)
• A multiverse of observer-containing worlds in which an infinite number of observer-moments are in each observer-moment. (In this model, an infinite number of observer-moments are in each observer-moment, and the number of observer-moments is finite.)
• A multiverse of observer-containing worlds in which all the observer-moments in each observer-moment are finite.
• An infinite number of observer-moments in each observer-moment.
• An infinite number of observer-moments in each observer-moment.
• An infinite number of observer-moments in each observer-moment.
• An infinite number of observer-moments in each observer-mom
====================
Pro-democracy activists who staged a sit-in in the Malaysian capital, Kuala Lumpur, last month called for the resignation of Prime Minister Najib Razak.
Razak was widely viewed as having failed to stand up to the demands of the pro-democracy movement. In particular, he had refused to support the referendum on increasing the minimum age for voting to 17 years. Protesters thought he had done nothing to reverse the tide of a "decade of constant stagnation" that they argued had taken place since the end of the Cold War.
But Razak, who is a prominent human rights icon, defended his record on voting rights. He said that in order to give the right to vote to all citizens who wish to vote, there would have to be a "25-year transition" that starts with the election of parliamentarians, and that it would then be "15 years" before a nationwide referendum on the right to vote.
The prime minister has since announced a moratorium on electoral reforms.
To what extent should the right to vote be given to all citizens? In practice, this is a complicated issue. In Malaysia, the principle of presumptive equality or suffrage rests on the assumption that voting is a right that can be exercised by everyone who is eligible to vote, and that voting should be made easier by the introduction of the voter-registration system. For people who cannot vote, such as those who are in prison or on death row, voting rights are granted only to those who can vote. In Malaysia, voting is made harder by the point system (where a voting card is counted in addition to having a photo identification card, and voters are required to mark their ballot on a piece of paper) used to count the number of their vote. For example, if there are more than four voting cards in a voter-card box, then at the end of the day four out of every five voters are required to cast a vote.
Regardless of how voting rights are granted, the view is that it should be made easier to vote by allowing voters to mark their ballot on a piece of paper, or by allowing people to cast provisional ballots that are counted in the same way as elections are held. In many countries, the voting cards used to count provisional ballots are made of plastic or cloth and are then handed to voters as they come in to the polling station. In Malaysia, voting cards made of cloth are also used to mark ballots, so that at the end of
====================
The Late Late Show with James Corden

James Corden: A Note to My Fellow Humans

http://www.jamescorden.com

(2008)

[pdf]
[translations available for download]

[pdf]

[translations in both HTML and PDF]

[translations in both HTML and PDF]

[translations in both HTML and PDF]

[translations in both HTML and PDF]

(2008)

[pdf]

                              

1. Introduction

Science fiction and fantasy are classic human imaginations. We are not just imagining them. We are living them. When we think of science fiction and fantasy, we imagine ourselves living in a parallel universe, or in a world where science and magic are real.

Today, science and technology are on the rise. The pace of technological development is such that, to put it mildly, things could go wrong very quickly. Fortunately, they don’t. If the first half of the 20th century was the peak for technological development, we are still living in the peak. Consider just a few of the technologies that are already on the horizon:

• Nuclear holocaust: The development of atomic weapons would likely lead to the end of life on Earth.

• Nanomedicine: Even in a world without nanotechnology, medicine would likely be able to cure every human disease.

• Nanoreplicators: If nuclear holocaust were to occur, it is unlikely that biological life would survive.

• Superintelligence: It may be possible to build a large number of intelligent species on Earth.

• Interstellar colonies: Colonizing other planets may well be the only way to go.

• Interstellar colonies: 2.1% of our planet’s total population is less than 1% of the total population of the cosmos.

• Artificial intelligence: It is possible to build an artificially intelligent civilization, perhaps on another planet, perhaps even in space.

• Human cloning: Human cloning could be a way to create a new human, or even a human clone.

• Artificial intelligence: Artificial intelligence is well underway. It is currently being developed at a pace that will make human cloning a
====================
This is a guest post by Chris Mooney. Chris is the original author of Extropy.com, which aims to be the largest open source information and analytics community on the web. His research focuses on the future of human civilization and the risks to global security that it poses. He is also the author of several books on the topic, including The End of the World (Routledge, 2010), The Coming Technological Singularity (Berkley Books, 2011), and Global Catastrophic Risks: When Computers, Minds, and Nations Collide (Oxford: OUP, 2011).

[Note: Chris Mooney is the author of the best-selling Extropy trilogy, which are now out in paperback (with an introduction by Christopher Mooney): Global Catastrophic Risks (Berkley Books, 2010)].

[/note]

Global catastrophic risks are not just theoretical; they are a reality.

Consider a simple experiment. Suppose that a laborer has to find a way to catch a fish that is swimming at a low level. The laborer has no way of knowing whether the fish is at or below this level, but if the laborer finds a way to catch the fish then the laborer will do the experiment.

The laborer may have no choice about the fish being at or below this level. The laborer may never have been in a position to make this kind of an experiment. The laborer has no way of knowing whether the laborer caught the fish or not, but if the laborer does the experiment then the laborer will have found a way to make it to the fish.

What about those who have no choice but to look for the fish? They might find themselves in a fish-filled sandbox. However, they may also have reason to think that the fish is not at such a low level that they did not look for a way to catch it. Maybe the laborer found a way to catch the fish, and the laborer did not look for a way to catch the fish, and then they find themselves in a fish-filled sandbox. Or perhaps one of them found a way to catch the fish, but the laborer did not look for a way to catch the fish, and then they find themselves in a fish-filled sandbox.

The laborer in the second example, the laborer who had no choice but to look for the fish,
====================
The half-life of a system is the time required for carbon dioxide to raise to atmospheric pressure. The average half-life of a gas like CO2 is about 20 hours. The half-life of an atom is about 2.7 hours. Given these parameters, a gas half-lives about 2.5 hours longer than a gas.
The reality is that the half-life of gas is much shorter than the half-life of atoms. The distance between a gas and a solid is much smaller than the distance between atoms and molecules. The half-life of a gas is therefore much shorter than the half-life of atoms.
Given the shortest half-life of gas (under 1 hour), this shows that a gas half-life of less than 1 hour will not have a significant impact on the overall half-life of the gas.
A gas half-life of less than 1 hour would have a significant impact only if the gas had a temperature of about -109.5C, close to the atmospheric temperature limit (T1). This temperature is not too far away from the equilibrium temperature of the gas.
The half-life of gases is very close to the half-life of atoms. It may therefore be reasonable to assume that gases half-lives are not important at all.
A gas half-life of less than 1 hour would, however, make a big difference if the temperature were 0C or higher. The half-life of gases is far greater than the half-life of atoms, so if the temperature were +109.5C, it would make a big difference if the gas half-life was 1 hour or longer.
If the half-life were 0C, a gas half-life of less than 1 hour would make a big difference even if the gas half-life was 1 hour. If the gas half-life was 1 hour, however, it would make an even bigger difference if the temperature was -109.5C, as the energy released from heating the gas by a flame would be much greater at -109.5C than at 109.5C.
Therefore, reducing the half-life of gas by a factor of two would cancel out the effect that would have been resulted from this action, and would therefore make no difference in the total half-life of the gas. The half-life of atoms, on the other hand, would increase to 2 hours.
According to the
====================
Intelligent observers who are not in a similar situation to ourselves can contribute to the picture by invoking “general principles” that we don’t use. These general principles can be formulated as a set of generalizations about the way the world works. If we put these generalizations in a large enough framework, they can be used to predict the future of the world.
Consider a hypothesis that the world will end in about 150 years. We would expect the number of people who will ever have existed to be very small. If this is true, then in the last 150 years, there will be a total of only about 10 billion people. The total number of people who will ever have existed is then only 10^17, which is a tiny fraction of the total population.
However, if we assume that the world ends in 150 years, and that we are the only intelligent observers, then the total number of people who will ever have existed is much larger. Suppose that we have extensive information about other civilizations. Suppose, for example, that we know that the number of people in the world is about 1.4 trillion. We might think that the number of people in the world is about 10^17 times greater than 10^17, and we should therefore assign a (very large) probability to the hypothesis that the world ends in 150 years.
The reason for this asymmetry in the expected number of people in the world is that the total number of people does not, strictly speaking, equal the total number of observers. The total number of observers, in fact, is about 1^32. But this number is an incomplete sum of the total number of observers. Therefore, the total number of observers does not equal the total number of people.
One way to fix this problem is by assigning a prior probability to the hypothesis that we are the only intelligent observers. This would make the total conditional probability that the world ends in 150 years that much larger. It would also imply that the probability that we are the only intelligent observers should be close to unity.
Another way to fix this problem is by asking whether there are other intelligent observers out there. This can be done by assigning a prior probability to the hypothesis that we are the only intelligent observers. This would make the total conditional probability that the world ends in 150 years that much smaller.
If we assign a prior probability to the hypothesis that we are the only intelligent observers, then T2 can be estimated as follows
====================
By

10/03/2006 10:51 AM Page 11
                                        11

In 1967, the then-president Lyndon B. Johnson announced a new space initiative, the National Aeronautics and Space Administration. The goal of the N. A. S. A. was to develop and launch a manned spaceflight by the mid-twentieth century. Although the program was conceived as a means to an end, the N. A. S. A. also came to be seen as a means to an unqualified success. From the outset, the N. A. S. A. was seen as a politically neutral enterprise: no one could claim that the N. A. S. A. was somehow involved in the formulation of any of the policy ideas or plans that resulted in the founding of the United States, the United Kingdom, the Commonwealth of Independent States, or the European Union.
On the N. A. S. A., the goal of space exploration was nothing less than a scientific and technological one. At the same time, space exploration was seen as a morally neutral endeavor: no one could say that the N. A. S. A. was somehow involved in the formulation of any of the scientific or technological priorities. A science and technology direction is not a science and technology direction unless the science and technology direction is, in fact, science and technology. Yet even when science and technology is involved, it is often not science and technology. Science and technology is often a manifestation of some related social and political trend, such as environmentalism, human enhancement, or the culture of technology. The N. A. S. A. was always in a position of being in conflict with such trends.
The N. A. S. A. often disdained the overtures of the technology-driven, environmentalist, human enhancement, or culture-of-technology trends. It was afraid of their contaminating the scientific and technological direction. The N. A. S. A. never quite managed to distance itself from these overtures. Yet it did try to keep its science and technology direction separate from its environmentalist or human enhancement commitments.
In 1971, the N. A. S. A. announced the goal of having a space colony
====================
Last week, the Wall Street Journal reported that, according to a study by two think-tankers, the number of people in China who estimate the annual cost of living to be between 2.5 and 7.5 trillion yuan ($30 to $100 trillion; £25 billion to $112 billion; €67 billion to €132 billion) has been on the rise since 1999.
On the whole, the researchers estimate that the cost of living in China in 2014 was between 2.1 trillion and 5.4 trillion yuan.
Assuming that the average Chinese person spends over 4,000 days a year in the country, the researchers estimate that the annual cost of living in China in 2014 was between 3.4 and 11.6 trillion yuan ($41.9 billion; £27.7 billion; €81.8 billion).
Since 1999, the number of days in China has risen by about 3,000 days per year.
This means that the study estimates that the number of people in China who estimate the annual cost of living in China and who think that the annual cost of living is between 2.5 and 7.5 trillion yuan has been increasing each year since 1999.
The study does not claim that this number is accurate, but the authors point out that it is an area where there is a gulf between what the average person thinks the cost of living is and what the average person thinks the annual cost of living is.
The paper suggests that people tend to overestimate the annual cost of living in China, and that this may be a contributing factor to the rise in the number of days in China reported as having been spent in the country.
In order to test this hypothesis, the researchers used a simple experiment they had done in the past. They taught a small group of people that their names, ages and birth ranks were the same as their birth ranks, and they were asked a series of random questions.
This experimental set-up produced a series of interesting results. One group was told their birth ranks would be 13,000; the other their would be 13,000,000 (13,000,001) years. The researchers were surprised to find that subjects who were told their birth ranks would be 13,000,000 years older than they actually were tended to overestimate their birth ranks.
One could of course argue that this result should not be surprising. Yet in the past, such an argument has not been very convincing. There has
====================
The US government is preparing to unveil a new set of surveillance plans that will allow the government to collect vast amounts of information on Americans without violating the law. The authorities will also propose a system of classification that will allow the government to combine this information with other information about Americans, such as photographs of the owners of cell phones.
The proposed global surveillance programme, codenamed PRISM, involves the collection of a huge amount of data on American citizens, including emails, text messages, and location information. The US government has also been engaged in a secret programme to improve its ability to predict terrorist plots, and to develop a new class of advanced technologies that will enable it to hack into computers and mobile devices via a worm.
While much of this information is exempt from disclosure to the public, a small cadre of right-wing activists and some media commentators are calling on the US government to reveal the methods by which these surveillance programmes are carried out. They claim that the mass surveillance programmes amount to an unconstitutional invasion of privacy. The US government has consistently maintained that this mass surveillance does not take place and that it is being conducted under lawful, and therefore constitutional, constraints.
Documents leaked by Edward Snowden in 2013 revealed that the NSA, in conjunction with a range of other agencies, was engaged in an enormous range of mass surveillance programmes. NSA programmes, it was claimed, included the collection of enormous quantities of domestic and international communications data, the systematic collection of personal data from internet and phone companies, the use of biometric identifiers such as iris scans, and the collection of meta-data that reveals the true location of everyone in the country.
The programme to collect the most powerful forms of intelligence information was codenamed Prism. The Prism documents show that the NSA and its British counterpart, the Government Communications Headquarters, are engaged in widespread mass surveillance of citizens, including the domestic communications of foreign targets, and the collection of vast quantities of personal data from innocent third parties.
The Prism documents, published by the German news magazine Der Spiegel, show that the NSA and GCHQ have been systematically collecting vast quantities of data on the domestic communications of foreign targets, without any explicit legal basis. The NSA is currently engaged in a major reform programme to limit its ability to conduct mass surveillance. The GCHQ documents also indicate that the NSA and GCHQ have been working on a project to build a computer network that they claim can match or exceed the performance of the best human brain. The NSA project was
====================
Mental imagery is a visual representation of the mental contents of a mind. Mental imagery can be activated or dis-activated by mental imagery, and is used to solve problems and fill in details in mental imagery. It is a non-local representation, and it is used to process and organize mental imagery.
A mental image is a representation of a mental state (or a mental concept) in a visual system. A mental image can be perceptually or semantically augmented to produce a mental image.
The brain is a simulation of the neurons in the brain. A simulation is a virtual simulation of a brain in a computer simulation. Simulation is a virtual object that can be moved in virtual space.
Simulation is a non-local representation in the computer system. The computer system simulates the brain in the same way, but only using local information about the simulation environment.
A mental image can be combined with simulated sensory experiences in a virtual reality. A virtual reality is a virtual reality where the senses and body are simulated. A virtual reality is a virtual world in which mental imagery can be created and experienced.
The brain takes place in a virtual reality, or a virtual reality. A virtual reality is a virtual environment in which a computer is simulating a brain. Every region of a virtual reality is simulated, and the physical world is simulated in the same way.
In some virtual reality, there are multiple realities that can be physically created. In these virtual realities, the same objects can be used to interact with the virtual world.
A virtual world can be divided into virtual universes, or virtual worlds. A virtual world is divided into virtual rooms, or virtual reality. A virtual reality is a virtual environment in which a computer is simulating a brain.
A virtual world could be divided into an initial virtual room, a virtual reality in which a computer is simulating a brain, and a virtual reality in which a computer is simulating a virtual world.
Virtual worlds may be created by a number of different methods.
One method is to create a virtual world from scratch. In this mode, a computer is simulating the body and brain of a virtual body in a virtual reality. The first virtual world may be generated by a simulation of the body and brain of the first virtual body; the second virtual world may be generated by a virtual body of the second virtual world. The computer can generate more virtual worlds by simulating more bodies and brains, or by simulating
====================
The results of our survey show that the majority of people responding to an online survey on the start of the cryptocurrency revolution (2012) believe that it will happen within five to ten years, with a median estimate of when it will happen of 2030.
This shows that people are not at all certain about the timing of the launch of the cryptocurrency revolution. Some people are even more uncertain, as has been the case in the past with other technologies such as the internet and e-mail.
Whilst we believe that it is unlikely that there will be a time when the majority of people believe that the start of the cryptocurrency revolution is the right time to put their money into cryptocurrencies, our survey, and our discussions with many others, suggest that it is not impossible that the question might be asked in the right context.
The timing of the start of the cryptocurrency revolution is of course of course only one of many different factors that should be taken into account in determining when to think about a launch (or other development) of cryptocurrencies.
Other factors such as the size and performance of the start-up community, opportunities for public investment, and the need to develop a more comprehensive understanding of the technological and economic trends relevant to the future of cryptocurrency and blockchain technology could also influence what is likely to be the right time for the launch of cryptocurrencies.
The timing of the start of the cryptocurrency revolution is thus part of a much wider set of considerations that must be considered in the planning of the cryptocurrencies that we will develop.
If you are an individual who has a significant amount of money to invest in cryptocurrency, and would like to learn more about where and when it is that your investment is likely to be spent, we invite you to send us an email at research@blockchain.info.
If you are a member of a trading institution that provides financial services to individuals or businesses that are interested in investing in cryptocurrencies, we invite you to send us an email at research@trading.info.
Our survey also asks respondents their views on what the probability of the cryptocurrency revolution being the right time to invest in cryptocurrencies. While many people think that the probability of the cryptocurrency revolution being the right time to invest in cryptocurrencies is very low (see figure 1), one can find evidence for this view through the comments on blogs and other sources. For example, on the one hand, many people, including some who are very confident that the crypto-currency market will close within the next year, think that
====================
The average American has about six hundred years of living experience. If you remember hunting and gathering for a long time in the mountains, then you likely have a thousand years of pre- vious experience of hunting, gathering, and producing food. This may be the most relevant life experience for an AI system.
If you are part of a diverse human Galactic population, then it may be more likely that you have several millennia of living experience of hunting, gathering, and producing food than it is that you will have one hundred and fifty years of living experience of hunting, gathering, and producing food.
If you consider the best-known programmers, scientists, and engineers, you may come to realize that the most likely group to have the greatest number of years of living experience are cosmologists, space scientists, and computer programmers. This group is more likely to be dominated by people with vastly more life experience because the vast majority of scientists, engineers, and computer scientists have a cosmic long-term vision.
This would explain why the most likely group to have the greatest life experience is the cosmology group. If you are a computer programmer, then the most likely group to have the greatest life experience is the group that is based on computer hardware. If you are a space scientist, then the most likely group to have the greatest life experience is the group that is based on space hardware. If you are a computer programmer, cosmologists, and space scientists, then the most likely group to have the greatest life experience is the group that is based on computers.
In summary, the most likely group to have the greatest life experience is the cosmology group.
It is not clear that there is an obvious way to tell which group you belong to. It could be (and probably is) an artifact of your birth rank, your ancestry, your gender, your education, your sexual orientation, the location of your birth, your life expectancy, etc. (And of course, we could forget you exist and then ask you about your birth rank.) It might also be possible to trace your actual ancestry to a group that is more closely related to the origin of your current brain than to the origin of your current brain. If so, then you probably belong in the more closely related group. But if you are part of at least one unrelated group, then you may well belong in a more closely related group—one in which case you probably belong in the more closely related group.
One may
====================
The idea of the “meta-level” is to give a non-indexical and non- indexical account of the world, and to allow for the possibility of non-indexical information. This allows the creation of a world where, for example, some information is nonindexical and nonindexical information can be used to create “meta- information”. The idea is that some mental states are more indexical than others, and that some mental states are more nonindexical than others. Thus, the meta-level approach can be combined with indexicalists to produce a nonindexical account of mental states.
The paradigmatic example of the meta-level approach is Chalmers’s “The Mind in the Machine: The Science of Mental Productivity,” (Chalmers, 1990). Chalmers uses a three-part approach to the problem of cognitive architectures: he argues that we cannot identify the specific cognitive architectures that lead to the emergent properties of the human mind that we have; we cannot link the emergent properties of the human mind to the emergent properties of the computer that is behind it; and we cannot develop concrete designs that make use of the emergent properties. He argues that for human minds to be able to be emergent, we must first identify the architectures that lead to the emergent properties of the mind.
The three parts in Chalmers’s account of emergent properties are that they do not exist solely in the specific cognitive architectures that make use of the emergent properties; we also need to understand how the emergent properties arise and how they are used to support the emergent properties. The meta-level approach to emergent properties is thus a means by which to describe emergent properties, and a means by which to develop a method for using them.
Another example of how the meta-level approach can be combined with indexicalists is in the area of deductive inference. In Feynman’s famous design problem, the Interdependency Problem, it was argued that it is possible to build a computer which will solve any two equations without first solving the third equation. This is because the equation of the three equations is indeterministic: the same three equations can be solved without a separate derivation for the third equation. The meta-level approach to reasoning with deductive inference can be formally described as an iterated version of the indexical approach to emergent
====================
This article is part of the IDS 2016 Data Science and Machine Intelligence workshop series.

Machine learning and artificial intelligence are becoming increasingly important for a wide range of industries, so much so that it is no longer enough to simply use existing algorithms to learn about the world around us. In the past, companies have used existing algorithms to recognize faces, write computer programs to help people read books, and predict stock market trends. Today, artificial intelligence is increasingly required to handle complex AI systems—from predicting (and in many cases deciding) when (and if) a breakthrough will occur to intelligently guiding the AI system to adjust its behavior to maximize its chances of survival.

There are currently two main approaches to AI research:

New methods are developed to help AI researchers become more competent. AI researchers develop methods and techniques that can be applied to large data sets. These new AI methods, which can be combined with the old methods, often lead to a “superintelligence”. Such a superintelligence, or “supercomputer,” would be the world’s largest computer, able to handle the task of running the AI system.

Developing AI methods to run on large data sets is a hugely complex undertaking, requiring extensive training and testing of computers to make sure that the AI system is working as intended.

The second approach is to use advanced AI methods to help AI researchers become better at predicting the future. This second approach focuses on the idea of “superintelligence”: that AI research should focus on developing algorithms that can learn and predict in unprecedentedly good detail the future of the world in a way that no other AI research can. This approach is driven by the need to build a body of knowledge that is both computationally efficient and theoretically tractable.

While neither approach is necessarily the most promising, they all have their strengths and weaknesses.

Suppose that the AI researchers are confident that their system is the best available and they do not believe that other AI researchers would be much better off with their current method. They are probably mistaken; it is quite likely that the AI system they develop would be superior to the AI system that would have been built by the AI researchers. A superintelligence would likely outstrip the AI researchers’ current system in performance and utility.

What the AI researchers should do is take a fresh look at their current method and ask what lessons there are to be learned from it.
====================
Presumptuous Scholar

                                                               
15 Ch 10 (159-184) 6/4/02 10:53 AM Page
183
                                                      
16 Ch 11 (185-206) 6/4/02 10:54 AM Page
184
                                                           
15 Ch 11 (185-206) 6/4/02 10:54 AM Page
185
                                                                                            
16 Ch 11 (185-206) 6/4/02 10:54 AM Page
186
                                                                                           
17 Ch 11 (186-206) 6/4/02 10:54 AM Page
187
                         
====================
If you have ever been in a car accident, or been in a hospital emergency room, you may have been stunned by a laser beam.

The laser can be used to create a very bright light source capable of blinding a person or other object from behind.

A bright laser can be used to blind a person or other object from behind

A laser is a massive, direct beam of light that is focused onto a target. It can be used to create a wide field of view (such as a laser dish) or extend the field of view by a telekinetic force or energy vector.

The laser is capable of killing anything in its path, including other people. The laser produces a large field of view with sufficient force to completely blind a person from behind.

It is, however, possible to blind a person from behind by physically blocking the light source. The mechanism by which this is done is not well understood.

An object that is blocked from the external world by another object that is physically blocked from the person in front of the

An object that is blocked from the interior world by another object that is physically blocked from the person in front.

A person in front of the object is not blinded, but is blinded by the external world.

The reverse process occurs: the object in front of the blind person is blinded by the object in front, while the person in front is not blinded.

This situation is analogous to the situation in which a person in front of an object that is blocked from the outside world is not blinded.

The laser light source blocks the outside world from the inside.

It is possible, however, to blind a person from behind by physically blocking the light source.

The person in front is not blinded by the external world, but is blinded by the external world plus the laser light source.

The person in front is no longer blinded by the outside world, but is blinded by the external world plus the laser light source.

The person in front can see the visual field of the object blocking the outside world, but cannot see the object blocking the person in front.

The person in front can see the external world, but cannot see the object blocking the person in front.

A person in the situation from which the external world is blocked can perceive the external world as a black void. A person in the situation from which the external
====================
H. R. Haldane

R. A. Lichtenstein

Lichtenstein, R. (2008). ‘Canterbury Cathedral in the Sky: The Unauthorized Story of Its Unification and Evolution.” Routledge/Kettle & Oswald.
[PDF]

                              
6. Discussion papers
(preprint arXiv:astro-ph/0704220v2)

                       
7. References
(available from the Internet Archive)

              (accessed 27 May 2016)

                                                                          
8. Appendix A: The Doomsday Argument and the Hypothesis of an Accelerating Universe from Supernovae
(available from the Internet Archive)

                The Doomsday Argument and the Hypothesis of an Accelerating Universe from Supernovae, H. R. Haldane, editor. R. A. Lichtenstein, Cambridge University Press, 2008.

                (accessed 27 May 2016)

                            
9. Appendix B: The Doomsday Argument and the Hypothesis of an Accelerating Universe from Supernovae
(available from the Internet Archive)

                       

                                                     
====================
THERE HAS BEEN A FEW LITTLE SCIENTIFIC USES OF PIRANHA AND NITROGEN. By S. J. C. and L. C. O. B. (eds.). Science and Philosophy: Philosophical Papers on the Transhumanist Frontier (New York: Routledge, 2003): pp. xi-xiii.

                                                                                                                  
(1) A SCIENTIFIC explanation of the origin of life and the origin of intelligence, offered as a hypothesis under which intelligent life arose from non-living matter in a manner which implies that intelligent life on Earth should have preceded the emergence of intelligent life on any other planet in the solar system. (Bostrom, 2003b)

                                                                              
(2) A SCIENTIFIC explanation of why the universe is fine-tuned to produce intelligent life. (Bostrom, 2003b)

                                                                                                                                
====================
An extraordinary new analysis from the distinguished and influential bioethicist and futurist Nick Bostrom argues that we are likely to have considerably more disruptive technological capabilities than we knew even a decade ago.
In his paper, "Artificial Intelligence: Paths, Dangers, Strategies," Bostrom outlines four broad paths to superintelligent machines: (1) Instrumentally constructing systems using artificial intelligence; (2) Instrumentally constructing systems using computer software; (3) Instrumentally constructing systems using neural networks; and (4) Automatically constructing systems using supercomputers.
The first two paths are technologically feasible but not yet industrializable. The third path would involve combining computer software with neural networks, and the fourth path would involve using supercomputing to construct systems.
Bostrom is a leading expert on the future of machine intelligence and co-founder of Singularity Institute. In this paper, he argues that, while this may not be the path of the most promising AI researchers, it is the path we ought to pursue in the future.
The first two paths are unlikely to be justly regarded as viable. They involve building more artificial intelligence. This would require a greater investment of resources and a larger subset of the human population would lose out. The human population, it would seem, is already overwhelming technologically. A further development of AI, in such a world, would require even more resources and would place even more people out of work.
One could argue that the faster AI path may be easier, and perhaps more morally and strategically viable, than the slower AI path. AI researchers may be able to produce more software in parallel than humans, which would enable them to more rapidly build up a technological lead over their competitors. Even if this is true, however, it does not make the slower AI path more attractive or theoretically viable, if the AI researchers have more resources to spend on research into AI than do humans.
If the AI path is economically viable and morally viable, it would require that AI be developed at a faster pace than humans. This would require advancing AI technology at least ten orders of magnitude faster than the human technology base.
The second path, which I shall call the “illusory” AI path, involves building AI before humans build AI. This is not the path of the most promising AI researchers, but it is the path of the people who are most likely to be harmed by AI. They may be the smartest people in
====================
Good morning media!

I'm going to take a moment today to address a thought experiment. I suggest that it is the case that a computer program that had been given a uniform distribution of instructions would be perfectly rational and would not make any suspicious assumptions. The thought experiment could be modified so that, instead of the program being perfectly rational, the citizens of a different continent could be brought into a different computer program and found to have committed the same number of crimes. The program they were brought into might have been programmed to follow the instructions exactly, but it would be perfectly rational for the program to not do so.

My introduction to the thought experiment is to explain how I see the problems of rational belief as part of a larger theory that identifies a set of credence assignments that, when used correctly, can produce a system of rational belief. This theory, which I will use to introduce the thought experiment, will postulate a multiverse that contains many universes and that these universes are quite unlike our own. In particular, I will argue that the thought experiment will postulate that there are many cases where the number of observers is very large. In such cases, I think it is possible to suggest that if the inhabitants of a multiverse exist, then it is probable that there will be one observer for each such universe in each observer-moment in the multiverse. If we would all have the same number of observers, then it is plausible to suppose that we should all observe the same multiverse. This would include many universes that do not have the same number of observers. Yet, if we have different numbers of observers and different numbers of observers in different universes, then it is plausible to suppose that we should all observe a multiverse.

My argument then, is that we are all in some sense omniscient and that we should assign a credence to every possible prediction that we could possibly make. This gives us the following probability assignment:

P(I am omniscient | I think I am omniscient | I am in an observer-moment that is not an observer-moment that is an observer-moment that is an observer-moment that is an observer-moment that is an observer-moment that is an observer-moment that is an observer-moment that is an observer-moment) = P(I am omniscient | I think I am omniscient | I am in an
====================
What is the difference between “common sense” and “objective reason”?
Let us try to define the difference. Objective reason is what the human mind uses to reason about the world. We can call this the “mind at work”. The human mind, by contrast, uses only the “objective” method to reason about the world. The objective method, by contrast, is the way the human mind looks at the world and uses it to arrive at objective reasons about reality.
Consider two examples of objective reason. In the first case, we will consider the objective method. Since objective reason relies on the mind at work for its method, we can consider the objective method as mathematically less rigorous than the objective method. In the second case, we will consider the objective method through the lens of“objective science”. In this second case, we will also consider the subjective and subjective method respectively. We will then see how in both cases, the results are remarkably similar. We will also see that the differences in the subjective and subjective methods are of the essence of the difference between objective and subjective reasons.
Let us first consider the objective method. First, we will consider a case in which the mind at work is the cosmology model. Then, we will consider a second case in which the mind at work is not the cosmology model. Then, we will consider a third case in which the mind at work is the multiverse model. Then, finally, we will consider a fourth case in which the multiverse model is correct. The aim here is to show how the use of objective and subjective methods can yield radically different answers.
In figure 3, we break down the main questions into three categories. The first three categories are based on the traditional probability function that functions as in (Gould and I. R. Lawrence, 1991), with P(x,y) denoted by the square root of P(y|x) and P(y|x^2). In the fourth category, we will introduce the new objective method that we call “the Doomsday Argument”. We then introduce a fourth category based on the new objective method that we call “the Anthropic Principle”. Finally, we consider other philosophical and methodological issues that will arise as we learn more about the nature of the reference class.
We will now consider the objective and subjective methods in turn. We
====================
“The Eureka Effect” is a scientific theory, popularized by the American cosmologist Richard Gott, that states that every observation we make is valid if and only if the observation is valid in the sense that it is true about the observed world. In this view, our observations are valid if and only if they are true about the world that we live in. The Eureka Effect asserts that the universe is like a small, closed system that, if there is any observer-moment that exists, will know the truth (Eureka). Rather than existing, the observer-moment would be created by another observer-moment that has come into existence by some other process. The effect is counterintuitive, since it implies that there must be a very big world out there; but it has been used by some traditional cosmologists to account for the large discrepancy between our observed world and the large discrepancy between their models and the real world, so it is not surprising when it is invoked by scientists who are not in favor of creating and sustaining observer-moments.
Gott’s theory is based on the idea that our observational selection effects are very weak and that one must wait for the evidence for an observation before one can be confident about its accuracy. Remarkably, this time-traveling cosmologist’s paradox does not even require a large universe to be the only theory that one should hold.
In the context of cosmology, however, this argument is incomplete. There are many theories that emphasize the existence of many observer-moments that come into existence and remain in existence and thus are able to observe the truth about the world. We can now see that they are not the only theories that count as observational predictions. We can also now show that the Eureka Effect is equally valid under other theories that say that our observational selection effects are very weak.
The case involving the Eureka Effect is analogous to the case involving the Doomsday Argument. The Doomsday Argument argues that if humanity is doomed, then there are some theories of how the world came about that imply that we will not be condemned to live in a world devoid of life. One such theory is the Big World theory, where the world is approximately 100 billion light years across and contains an infinite number of universes. This theory, unlike the Eureka Effect, states that our observation selection effects are very weak.
This paper, therefore, will argue
====================
This is the reply to an email question on the meaning of “Theorem 3. Theorem 1. We have the following information:
1.                  
2.               3.   
4.  
5.  
I have a feeling that there is a misunderstanding here, but I need to talk to you about it, so I will try to be as succinct and to the point as possible.
Theorem 1 is true if we know the following:
(1)             
(2)              
(3)             
(4)             
(5) 
             
(6)  
  

We can derive theorem 2 using this information:
(2b)                 (2c)                  (2d)                
(3)                  
(4)                    (5)                     
(6)                   
(7)                   
(8)                   
(9)                
(10)               
(11)                
(
====================
In a search for targets for use in climate modelling, the GFDL has visualized several possible combinations of temperature, precipitation, and elevation. They have found that the average global warming of the past century is very likely (and probably) to be between -1 and +4C (see Figure 1), and that this warming is likely to be amplified by factors such as soil moisture and volcanic aerosols. Given that the total warming of the Earth, by far, is likely to be much greater than +4C, this amplification could have consequences for the Earth’s temperature by several orders of magnitude (a factor of 10-fold in absolute terms). The GFDL has also found that if the Earth is forced to warm by around 4C, the resulting warming would likely lead to major changes in the Earth’s physics and ecology, making changes to the landscape of the planet such as changes in temperature and precipitation more likely. The GFDL’s best estimate for the temperature change during the Holocene (the last ice age period) is around +1.5C. The GFDL’s most recent global warming estimate is around +2.5C. Hence, assuming a range of different values for temperature and precipitation, and assuming that sea level and ozone depletion are ignored, the GFDL estimates would lead to a warming by +2.5C, assuming that the change in temperature and precipitation causes no damage to the Earth’s ozone layer.
Figure 1: Comparison of temperature and precipitation forecasts for the GFDL (left panel) and the US GISS (right panel). Note that both these forecasts are subject to major uncertainties.

The largest uncertainty lies in the forecast of the amount and type of precipitation and the forecast of global temperature. These forecast uncertainties are particularly large given that the GFDL does not specify a time-scale for the precipitation forecast. The GFDL is therefore bound to make some forecast errors. The forecast of global temperature presumably needs as much or more uncertainty than does the forecast of precipitation, although it is unclear how much of this is due to the nature of the climate and how much to do with the fact that the GFDL does not specify a time-scale for the forecast.
Figure 1: The GFDL and US GISS water level forecasts.

In the following we will use the term “weather event” to refer to any event that causes the area covered by a body of water to change
====================
JANUARY 22 // BISU

New York

Jennifer Granick, Brendan Nyhan, and Sam Altman

Abstract

This paper explores the implications for public policy of nuclear weapons. It argues that, in a world where there is a large number of nuclear weapons, the use of such weapons could conceivably increase the threat level by allowing more countries to kill each other, thus reducing the overall threat level. The paper also argues that a more nuclear-weapon-concealed world would increase the probability that nuclear weapons would be used. We also consider the consequences of a nuclear-weapons-free world for international security, and how these effects could be mitigated. Finally, we consider a set of possible additional security risks that could result from a more nuclear-weapons-concealed world.

Keywords
Nuclear weapons
Global catastrophic risks
Nuclear deterrence
International security
Nuclear nonproliferation
Nuclear arms race
Nuclear arms race
Global catastrophic risks
The Doomsday Argument
The Doomsday Argument proposes that if the world’s population continues to grow at the rate of nearly one million people per minute, it will unavoidably reach the end of its productive capacity in approximately 2140. Some conservative estimates place the probability of this finding being confirmed, assuming that there are about 100,000 nuclear weapons in the world, even if we abdicate our responsibility to avoid their arsenals, at roughly one in ten thousand, that the world will be destroyed. The Doomsday Argument is a straw man of a twist on the more familiar Doomsday argument (discussed in some detail in [1]). But it is not the kind of twist that has gained wider attention in recent years.

Somehow, by some pre-set criteria, we have managed to make it to the end of humanity’s productive capacity. Those who use this argument do not deny that it is probable that our species will one day perish. They merely claim that the world’s population density at the present time, and the fact that it is currently very, very large, are insufficient grounds for elevating the probability that we will perish. This view, however, ignores the fact that there are many possible outcomes that could turn out not to be particularly bad. We may be on the verge of a technological revolution that could bring about the end of the human species. This is easy to show.

One could therefore begin by setting
====================
A new study in Science finds that the Earth is heading towards a cataclysm.
The study, entitled “The End of the World: The Science and Ethics of Human Extinction”, was led by Nick Bostrom, one of the leading futurologists, and is the most comprehensive assessment yet of the global strategic picture. There are several themes that arise from this work. First, many observers are skeptical about the probability of human extinction in the near future. The concept of singularity – a time where technological development would make it possible to create a “posthuman” world – has become increasingly controversial as artificial intelligence and nanotechnology have become more closely interconnected (see Figure 1).

          

         

         

                  
Fig. 1
The next few decades will be critical for the survival and flourishing of humanity. As soon as human civilisation has risen sufficiently to make a difference to the environment, then the threat of extinction becomes a vanishing concern. But as soon as such civilisation has become too powerful and oppressive, then it may turn out to be a necessity for all people to remain in constant state of constant fear, uncertainty, and anxiety about the consequences of their actions. This brings us to the second theme that emerges from the study of the future of human civilisation: that we might not have any time to think about the future at all.

         

             

                    
                 
                
                    

               

                                    

                          

====================

`;

const poem = `The metaphor of the rod and the stick does not fit the current situation
There are many other equally plausible explanations for why our universe is the way it is.

we are living in a computer simulation that's been designed to produce "weird results"

we imagine ourselves living in a parallel universe,
or in a world where science and magic are real.

The sense of being connected to an external world is a fundamental part of intelligence.
A person in the situation from which the external world is blocked can perceive the external world as a black void.

What is the difference between "common sense" and "objective reason"?
we are all in some sense omniscient

In the real world, road illumination is often a function of time.
The field of view may be shifted by the agent's own motion
This time-constraint is how roadway markings, traffic signals, and other such intrinsic features of the road are
An infinite number of observer-moments in each observer-moment.

A mental image can be combined with simulated sensory experiences in a virtual reality.
In these virtual realities, the same objects can be used to interact with the virtual world.

A virtual world is divided into virtual rooms
the concept of an "interactive space"
specific cognitive architectures
architectures that lead to the emergent properties of the mind.

The Eureka Effect asserts that the universe is like a small, closed system that, 
if there is any observer-moment that exists, 
will know the truth (Eureka).`;


    const botTextContainer = document.createElement('div');
    botTextContainer.classList.add('botText');
    botTextContainer.innerHTML = botText;
    const poemContainer = document.createElement('div');
    poemContainer.classList.add('poemContainer');
    const poemText = document.createElement('div');
    poemText.classList.add('poemText');
    poemText.innerHTML = poem;

    document.body.append(botTextContainer);
    document.body.append(poemContainer);
    poemContainer.append(poemText);

</script>

</html>